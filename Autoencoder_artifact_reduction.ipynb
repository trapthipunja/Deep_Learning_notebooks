{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=blue>Artifact reduction network</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import time\n",
    "from torch import nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load patches of data for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_poly_train = torch.load('./patch_seg_poly_LR.pt')\n",
    "patch_mono_train = torch.load('./patch_seg_monolabel_LR.pt')\n",
    "patch_poly_val = torch.load('./patch_seg_poly_val_LR.pt')\n",
    "patch_mono_val = torch.load('./patch_seg_monolabel_val_LR.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PCBcustomDataset(data.Dataset):\n",
    "        \"\"\"Characterizes PCB dataset\"\"\"\n",
    "    def __init__(self, patches_data, patches_label):\n",
    "        self.patches_data = patches_data\n",
    "        self.patches_label = patches_label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Denotes the total number of PCB samples\"\"\"\n",
    "        return len(self.patches_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generates one sample of PCB data\"\"\"\n",
    "        #select sample\n",
    "        x_data = self.patches_data[index]\n",
    "        y_label = self.patches_label[index]\n",
    "        # Unsqueeze channel dimension\n",
    "        x_data = x_data.unsqueeze(0)\n",
    "        y_label = y_label.unsqueeze(0)\n",
    "        return x_data, y_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise the patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "Volume =0 #patch \n",
    "slicee =9 #slice in the patch \n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2)\n",
    "im = ax1.imshow(patch_poly_train[Volume,slicee,:,:], cmap='gray')\n",
    "im2 = ax2.imshow(patch_mono_train[Volume,slicee,:,:], cmap='gray')\n",
    "ax1.title.set_text('Input')\n",
    "ax2.title.set_text('Actual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( patch_poly_train.shape, patch_poly_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_pcb_train = PCBcustomDataset((patch_poly_train), (patch_mono_train))\n",
    "dataset_pcb_val = PCBcustomDataset((patch_poly_val),(patch_mono_val))\n",
    "print('train directory has {} samples'.format(len(dataset_pcb_train)))\n",
    "print('val directory has {} samples'.format(len(dataset_pcb_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainloader = data.DataLoader(dataset_pcb_train, batch_size=4, shuffle= True, num_workers=1,drop_last= False)#4\n",
    "valloader =data.DataLoader(dataset_pcb_val, batch_size=4, shuffle= True, num_workers=1,drop_last= False)#4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder_VARN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder_VARN, self).__init__()\n",
    "        # encoder\n",
    "        self.downlayer1 = nn.Sequential(nn.Conv3d(1, 12, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 2, 2)),\n",
    "                                        nn.ReLU())\n",
    "        self.downlayer2 = nn.Sequential(nn.Conv3d(12, 24, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 2, 2)),\n",
    "                                        nn.ReLU())\n",
    "        self.downlayer3 = nn.Sequential(nn.Conv3d(24, 48, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 2, 2)),\n",
    "                                        nn.ReLU())\n",
    "        self.downlayer4 = nn.Sequential(nn.Conv3d(48, 96, kernel_size=(3, 5, 5), padding=(1, 2, 2), stride=(1, 2, 2)),\n",
    "                                        nn.ReLU())\n",
    "\n",
    "        # decoder\n",
    "        self.bottleneck = nn.Sequential(nn.Conv3d(96, 96, kernel_size=(3, 5, 5), padding=(1, 2, 2)),\n",
    "                                        nn.ReLU())\n",
    "        self.aux_conv = nn.Sequential(nn.Conv3d(96, 1, kernel_size=(3, 5, 5), padding=(1, 2, 2)),\n",
    "                                      )\n",
    "        self.uplayer0 = nn.Sequential(nn.Conv3d(96, 48, kernel_size=(3, 5, 5), padding=(1, 2, 2)),\n",
    "                                      nn.ReLU())\n",
    "        self.uplayer1 = nn.Sequential(nn.Conv3d(48, 24, kernel_size=(3, 5, 5), padding=(1, 2, 2)),\n",
    "                                      nn.ReLU())\n",
    "        self.upsample1 = nn.Upsample(size=(5, 125, 125))\n",
    "        self.uplayer2 = nn.Sequential(nn.Conv3d(24, 12, kernel_size=(3, 5, 5), padding=(1, 2, 2)),\n",
    "                                      nn.ReLU())\n",
    "        self.upsample2 = nn.Upsample(scale_factor=(1, 2, 2), mode=\"trilinear\", align_corners=True)\n",
    "        self.uplayer3 = nn.Sequential(nn.Conv3d(12, 1, kernel_size=(3, 5, 5), padding=(1, 2, 2)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_original = x\n",
    "        d, h, w = x_original.shape[2:]\n",
    "        x = self.downlayer1(x)\n",
    "        x_d1 = x\n",
    "        x = self.downlayer2(x)\n",
    "        x_d2 = x\n",
    "        x = self.downlayer3(x)\n",
    "        x_d3 = x\n",
    "        x = self.downlayer4(x)\n",
    "        x = self.bottleneck(x)\n",
    "        x_aux = self.aux_conv(x)\n",
    "        x_aux = F.interpolate(x_aux, size=(d, h, w), mode=\"trilinear\", align_corners=True)\n",
    "        x = F.interpolate(x, size=(x_d3.shape[2], x_d3.shape[3], x_d3.shape[4]), mode=\"trilinear\", align_corners=True)\n",
    "        x = self.uplayer0(x)\n",
    "        x = x + x_d3\n",
    "        x = F.interpolate(x, size=(x_d2.shape[2], x_d2.shape[3], x_d2.shape[4]), mode=\"trilinear\", align_corners=True)\n",
    "        x = self.uplayer1(x)\n",
    "        x = x + x_d2\n",
    "        x = F.interpolate(x, size=(x_d1.shape[2], x_d1.shape[3], x_d1.shape[4]), mode=\"trilinear\", align_corners=True)\n",
    "        x = self.uplayer2(x)\n",
    "        x = x + x_d1\n",
    "        x = F.interpolate(x, size=(d, h, w), mode=\"trilinear\", align_corners=True)\n",
    "        x = self.uplayer3(x)\n",
    "        x += x_original\n",
    "        if self.training:\n",
    "            return x, x_aux\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv3d):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = nn.DataParallel(Autoencoder_VARN().cuda())\n",
    "net.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "costfunction = nn.SmoothL1Loss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=--, weight_decay=--)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSIM metric\n",
    "from pytorch_msssim import *\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('--- Training of VARN--- ')\n",
    "training_start_time = time.time()\n",
    "num_epochs = 500\n",
    "weight = 0.3\n",
    "train_losses, val_losses = [], []\n",
    "train_acc, val_acc = [], []\n",
    "val_loss_temp = 1\n",
    "for e in tqdm(range(num_epochs)):\n",
    "    logs = {}\n",
    "    running_loss = 0\n",
    "    ssim_train = 0\n",
    "    net.train()\n",
    "    for i, data_samples in enumerate(trainloader):\n",
    "        volume, labels = data_samples\n",
    "        volume = volume.cuda()\n",
    "        labels = labels.cuda()\n",
    "\n",
    "        output, output_aux = net(volume.float())\n",
    "        loss1 = costfunction(output, labels.float())\n",
    "        loss_aux = costfunction(output_aux, labels.float())\n",
    "        loss = loss1 + weight * loss_aux\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    else:\n",
    "        val_loss = 0\n",
    "        ssim_val = 0\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            volume, labels = next(iter(valloader))\n",
    "            for i, data_samples in enumerate(valloader):\n",
    "                volume, labels = data_samples\n",
    "                volume = ((volume).cuda())\n",
    "                labels = labels.cuda()\n",
    "                outputs = net(volume)\n",
    "                val_loss += costfunction(outputs, labels)\n",
    "\n",
    "                accuracy = SSIM_accuracy(outputs, labels, data_range=labels.max() - labels.min())\n",
    "                ssim_val += accuracy.item()\n",
    "\n",
    "        # save best model        \n",
    "        if (val_loss / len(valloader)) < val_loss_temp:\n",
    "            torch.save({\n",
    "                'epoch': e,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': (val_loss / len(valloader)),\n",
    "            }, './AR_aux_loss_bestval_LR.pt')  \n",
    "            val_loss_temp = (val_loss / len(valloader))\n",
    "\n",
    "        train_losses.append(running_loss / len(trainloader))\n",
    "        val_losses.append(val_loss / len(valloader))\n",
    "        train_acc.append(ssim_train / len(trainloader))\n",
    "        val_acc.append(ssim_val / len(valloader))\n",
    "        print(\"Epoch: {}/{}.. \".format(e + 1, num_epochs),\n",
    "              \"Training Loss: {:.3f}.. \".format(running_loss / len(trainloader)),\n",
    "              \"val Loss: {:.3f}.. \".format(val_loss / len(valloader)),\n",
    "              \"SSIM metric: {:.3f}.. \".format(ssim_val / len(valloader)))\n",
    "\n",
    "# save complete model        \n",
    "torch.save(net.state_dict(), './AR_aux_loss_LR.pt')\n",
    "print('Training finished in {}'.format(time.time() - training_start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(train_losses, label=\"Training Loss\")\n",
    "ax.plot(val_losses, label=\"Validation Loss\")\n",
    "ax.set_xlabel(\"epochs\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.set_title(\"Loss vs Epochs\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = nn.DataParallel(Autoencoder_VARN().cuda())\n",
    "checkpoint = torch.load('./AR_aux_loss_bestval_LR.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_pcb_test = PCBcustomDataset((patch_poly_val),(patch_mono_val))\n",
    "testloader =data.DataLoader(dataset_pcb_test, batch_size=1,shuffle=False, num_workers=1,drop_last= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss(reduction ='mean')\n",
    "testloss, ssim, mse,smoothl1 = [],[],[],[]\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, data_samples in tqdm(enumerate(testloader)):\n",
    "        volume, labels = data_samples\n",
    "        volume = ((volume).cuda())\n",
    "        labels = labels.cuda()\n",
    "        outputs = model(volume)\n",
    "        ssim_accuracy = SSIM_accuracy(outputs, labels, data_range=20)\n",
    "        print(ssim_accuracy)\n",
    "        ssim.append(ssim_accuracy.detach().cpu())\n",
    "        mse_accuracy = mse_loss(outputs, labels)\n",
    "        smoothl1_loss = costfunction(outputs, labels)\n",
    "        smoothl1.append(smoothl1_loss.detach().cpu())\n",
    "        mse.append(mse_accuracy.detach().cpu())\n",
    "print('Mean SSIM accuracy is {}'.format(np.mean(ssim), np.std(ssim)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "slicee = 20\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,3,2)\n",
    "ax3 = fig.add_subplot(1,2,2)\n",
    "im = ax1.imshow(pred_poly[0,0,slicee,:,:],cmap='gray')\n",
    "clim=im.properties()['clim']\n",
    "ax2.imshow(pred_mono[0,0,slicee,:,:], clim=clim, cmap='gray')\n",
    "ax3.imshow(prediction[0,0,slicee,:,:].cpu().detach().numpy(), clim = clim, cmap= 'gray')\n",
    "fig.colorbar(im, ax=(ax1,ax2,ax3), shrink=0.2)\n",
    "ax1.title.set_text('Artifact volume')\n",
    "ax2.title.set_text('Ground truth')\n",
    "ax3.title.set_text('Predicted')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
